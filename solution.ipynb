{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb9e531",
   "metadata": {},
   "source": [
    "# Утилиты для загрузки/очистки/разбиения датасета. (data_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22eec05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "data_utils.py\n",
    "Утилиты для загрузки/очистки/разбиения датасета.\n",
    "\n",
    "структура:\n",
    "- data/raw_dataset.csv\n",
    "- data/dataset_processed.csv\n",
    "- data/train.csv, data/val.csv, data/test.csv\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# пути\n",
    "RAW_PATH = \"data/raw_dataset.csv\"\n",
    "PROCESSED_PATH = \"data/dataset_processed.csv\"\n",
    "TRAIN_PATH = \"data/train.csv\"\n",
    "VAL_PATH = \"data/val.csv\"\n",
    "TEST_PATH = \"data/test.csv\"\n",
    "\n",
    "\n",
    "def clear_datatset():\n",
    "    # Очистка набора. Согласно заднию нужно так сделать,\n",
    "    # но скорее всего используемый мной токенизатор позволяет корректно учиться и без этого\n",
    "    with open(RAW_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        texts = [line.rstrip(\"\\n\") for line in f]\n",
    "\n",
    "    df = pd.DataFrame({\"text\": texts})\n",
    "\n",
    "    def _clean_text(text):\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        text = text.lower()  # к нижнему регистру\n",
    "        text = re.sub(r\"[^a-z0-9 ]+\", \" \", text)  # оставить только буквы и цифры\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()  # убрать дублирующиеся пробелы\n",
    "        return text\n",
    "\n",
    "    df_clean = df[\"text\"].map(_clean_text)\n",
    "    df_clean = df_clean[df_clean.str.len() > 0].reset_index(drop=True) # выкинем пустые после очистки\n",
    "\n",
    "    df_clean.to_csv(PROCESSED_PATH, index=False, header=False)\n",
    "\n",
    "    return df_clean\n",
    "\n",
    "\n",
    "def split_dataset(df_clean, test_size=0.8):\n",
    "    # разбиение набора на части\n",
    "    train_df, tmp_df = train_test_split(\n",
    "        df_clean,\n",
    "        test_size=1-test_size,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_df, test_df = train_test_split(\n",
    "        tmp_df,\n",
    "        test_size=0.5,\n",
    "        random_state=42,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    train_df.to_csv(TRAIN_PATH, index=False, header=False)\n",
    "    val_df.to_csv(VAL_PATH, index=False, header=False)\n",
    "    test_df.to_csv(TEST_PATH, index=False, header=False)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def prepare_all_data(force: bool = False, debug_test: bool = False):\n",
    "    # Основная функция для подготовки наборов данных\n",
    "    if (\n",
    "        not force\n",
    "        and not debug_test\n",
    "        and os.path.isfile(TRAIN_PATH)\n",
    "        and os.path.isfile(VAL_PATH)\n",
    "        and os.path.isfile(TEST_PATH)\n",
    "    ):\n",
    "        print(\"All data files already exist, simple read\")\n",
    "\n",
    "        train_df = pd.read_csv(TRAIN_PATH, header=None)\n",
    "        val_df = pd.read_csv(VAL_PATH, header=None)\n",
    "        test_df = pd.read_csv(TEST_PATH, header=None)\n",
    "    \n",
    "    else:\n",
    "        print(\"Create cleared data file\")\n",
    "        df = clear_datatset()\n",
    "\n",
    "        # Если отладка, то для теста оставить в наборе только первые 20000 строк\n",
    "        if debug_test:\n",
    "            df = df.head(20000)\n",
    "            print(f\"First 5 texts:\\n{df.head()}\\ntotal length = {len(df)}\\n\")\n",
    "\n",
    "        print(\"Split data files\")\n",
    "        train_df, val_df, test_df = split_dataset(df)\n",
    "    \n",
    "    print(f\"Lenght: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     prepare_all_data(force=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e2513",
   "metadata": {},
   "source": [
    "# Torch Dataset для задачи next-token prediction (автодополнение текста). (next_token_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60e65b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "next_token_dataset.py\n",
    "Torch Dataset для задачи next-token prediction (автодополнение текста).\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class NextTokenDataset(Dataset):\n",
    "    def __init__(self, ids_list, eos_id):\n",
    "        self.ids_list = ids_list\n",
    "        self.eos_id = eos_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ids = self.ids_list[idx]\n",
    "        ids = torch.tensor(ids, dtype=torch.long)\n",
    "\n",
    "        if ids.numel() < 2:\n",
    "            ids = torch.tensor([self.eos_id, self.eos_id], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": ids[:-1],  # X\n",
    "            \"labels\": ids[1:]       # Y (сдвиг вправо)\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch, pad_id: int):\n",
    "    xs = [item[\"input_ids\"] for item in batch]\n",
    "    ys = [item[\"labels\"] for item in batch]\n",
    "    lengths = torch.tensor([len(x) for x in xs], dtype=torch.long)\n",
    "\n",
    "    x_pad = pad_sequence(xs, batch_first=True, padding_value=pad_id)\n",
    "    y_pad = pad_sequence(ys, batch_first=True, padding_value=-100)  # для ignore_index\n",
    "\n",
    "    attention_mask = (x_pad != pad_id).long()\n",
    "\n",
    "    return {\"input_ids\": x_pad, \"attention_mask\": attention_mask, \"labels\": y_pad, \"lengths\": lengths}\n",
    "\n",
    "\n",
    "def GenDataLoaders(train_df, val_df, test_df, tokenizer):\n",
    "    # Создает Torch Dataset-ы для задачи next-token prediction (автодополнение текста)\n",
    "    # tokenizer берется как входной, чтобы он был инициализирован одинаково для будущего сравнения с трансформером\n",
    "    \n",
    "    # претокенизация\n",
    "    train_ids = tokenizer(list(train_df), truncation=True, max_length=128, padding=False)[\"input_ids\"]\n",
    "    val_ids   = tokenizer(list(val_df),   truncation=True, max_length=128, padding=False)[\"input_ids\"]\n",
    "    test_ids  = tokenizer(list(test_df),  truncation=True, max_length=128, padding=False)[\"input_ids\"]\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "\n",
    "    train_ds = NextTokenDataset(train_ids, eos_id=eos_id)\n",
    "    val_ds   = NextTokenDataset(val_ids,   eos_id=eos_id)\n",
    "    test_ds  = NextTokenDataset(test_ids,  eos_id=eos_id)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=256, shuffle=True,\n",
    "                            collate_fn=lambda b: collate_fn(b, pad_id=pad_id))\n",
    "    val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False,\n",
    "                            collate_fn=lambda b: collate_fn(b, pad_id=pad_id))\n",
    "    test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False,\n",
    "                            collate_fn=lambda b: collate_fn(b, pad_id=pad_id))\n",
    "    \n",
    "    return train_loader, val_loader, test_loader \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc1dce",
   "metadata": {},
   "source": [
    "# LSTM-модель для языкового моделирования (next token). (lstm_model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fab3eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lstm_model.py\n",
    "LSTM-модель для языкового моделирования (next token).\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class LSTMNextToken(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int = 128,\n",
    "        hidden_size: int = 256,\n",
    "        num_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "        pad_id: int = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        \"\"\"\n",
    "        input_ids: [B, T] (X)\n",
    "        lengths:   [B]\n",
    "        return logits: [B, T, V] — предсказание следующего токена для каждого t\n",
    "        \"\"\"\n",
    "        emb = self.embedding(input_ids)  # [B,T,E]\n",
    "\n",
    "        packed = pack_padded_sequence(\n",
    "            emb, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # [B,T,H]\n",
    "\n",
    "        logits = self.fc(out)  # [B,T,V]\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        prefix_ids: torch.Tensor,      # [T] или [1,T]\n",
    "        lengths: torch.Tensor | None = None,\n",
    "        max_new_tokens: int = 20,\n",
    "        eos_id: int | None = None,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int | None = 50,\n",
    "        do_sample: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Возвращает ids = prefix + продолжение.\n",
    "        Простой авторегрессионный цикл: каждый раз берём logits последней позиции.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        if prefix_ids.dim() == 1:\n",
    "            ids = prefix_ids.unsqueeze(0).to(device)  # [1,T]\n",
    "        else:\n",
    "            ids = prefix_ids.to(device)\n",
    "\n",
    "        if lengths is None:\n",
    "            lengths = torch.tensor([ids.size(1)], device=device)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(ids, lengths=lengths)         # [1,T,V]\n",
    "            next_logits = logits[:, -1, :]                      # [1,V]\n",
    "\n",
    "            if temperature != 1.0:\n",
    "                next_logits = next_logits / max(temperature, 1e-8)\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, ix = torch.topk(next_logits, k=top_k, dim=-1)\n",
    "                filt = torch.full_like(next_logits, float(\"-inf\"))\n",
    "                filt.scatter_(1, ix, v)\n",
    "                next_logits = filt\n",
    "\n",
    "            if do_sample:\n",
    "                probs = F.softmax(next_logits, dim=-1)\n",
    "                next_id = torch.multinomial(probs, num_samples=1)  # [1,1]\n",
    "            else:\n",
    "                next_id = torch.argmax(next_logits, dim=-1, keepdim=True)  # [1,1]\n",
    "\n",
    "            ids = torch.cat([ids, next_id], dim=1)  # [1, T+1]\n",
    "            lengths = lengths + 1\n",
    "\n",
    "            if eos_id is not None and next_id.item() == eos_id:\n",
    "                break\n",
    "\n",
    "        return ids.squeeze(0)  # [T_total]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e7f55",
   "metadata": {},
   "source": [
    "# Оценка LSTM модели: код замера и вывода метрики ROUGE. (ROUGE + генерация 3/4→1/4) (eval_lstm.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b122f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "eval_lstm.py\n",
    "Оценка LSTM модели: код замера и вывода метрики ROUGE. (ROUGE + генерация 3/4→1/4).\n",
    "\"\"\"\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "def _ids_to_text(tokenizer, ids):\n",
    "    # ids: 1D tensor/list\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        ids = ids.tolist()\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_rouge_3of4(\n",
    "    model,\n",
    "    dataloader,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    max_new_tokens: int = 32,\n",
    "    do_sample: bool = False,\n",
    "    top_k: int | None = None,\n",
    "    temperature: float = 1.0,\n",
    "    limit_batches: int | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Сценарий: берем полный текст (input+labels), режем на prefix=3/4 и target=1/4.\n",
    "    Генерируем продолжение для prefix и считаем ROUGE между generated_suffix и target.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\"], use_stemmer=True)\n",
    "\n",
    "    total_r1, total_r2, n = 0.0, 0.0, 0\n",
    "\n",
    "    for bi, batch in enumerate(dataloader):\n",
    "        if limit_batches is not None and bi >= limit_batches:\n",
    "            break\n",
    "\n",
    "        x = batch[\"input_ids\"].to(device)         # [B,T]\n",
    "        y = batch[\"labels\"].to(device)            # [B,T] (с -100 на паддингах)\n",
    "        lengths = batch[\"lengths\"].to(device)     # [B]\n",
    "\n",
    "        B = x.size(0)\n",
    "        for i in range(B):\n",
    "            L = int(lengths[i].item())\n",
    "            if L < 4:\n",
    "                continue\n",
    "\n",
    "            # восстановим \"полную\" последовательность ids длины L+1:\n",
    "            # full = [x0..x_{L-1}] + [y_{L-1}]\n",
    "            last_target = y[i, L - 1].item()\n",
    "            if last_target == -100:\n",
    "                continue\n",
    "\n",
    "            full_ids = torch.cat(\n",
    "                [x[i, :L], torch.tensor([last_target], device=device, dtype=torch.long)]\n",
    "            )  # [L+1]\n",
    "\n",
    "            cut = max(1, int(0.75 * full_ids.numel()))\n",
    "            prefix_ids = full_ids[:cut]     # 3/4\n",
    "            target_ids = full_ids[cut:]     # 1/4\n",
    "\n",
    "            # генерим не больше, чем длина таргета (чтобы сравнение было честнее)\n",
    "            gen_len = min(max_new_tokens, int(target_ids.numel()))\n",
    "            gen_ids = model.generate(\n",
    "                prefix_ids=prefix_ids,\n",
    "                lengths=torch.tensor([prefix_ids.numel()], device=device),\n",
    "                max_new_tokens=gen_len,\n",
    "                eos_id=tokenizer.eos_token_id,\n",
    "                temperature=temperature,\n",
    "                top_k=top_k,\n",
    "                do_sample=do_sample,\n",
    "            )\n",
    "\n",
    "            gen_suffix = gen_ids[prefix_ids.numel():]  # только сгенерированное продолжение\n",
    "\n",
    "            ref_text = _ids_to_text(tokenizer, target_ids)\n",
    "            hyp_text = _ids_to_text(tokenizer, gen_suffix)\n",
    "\n",
    "            if len(ref_text) == 0 or len(hyp_text) == 0:\n",
    "                continue\n",
    "\n",
    "            scores = scorer.score(ref_text, hyp_text)\n",
    "            total_r1 += scores[\"rouge1\"].fmeasure\n",
    "            total_r2 += scores[\"rouge2\"].fmeasure\n",
    "            n += 1\n",
    "\n",
    "    if n == 0:\n",
    "        return {\"rouge1_f\": 0.0, \"rouge2_f\": 0.0, \"n_samples\": 0}\n",
    "\n",
    "    return {\"rouge1_f\": total_r1 / n, \"rouge2_f\": total_r2 / n, \"n_samples\": n}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8659e",
   "metadata": {},
   "source": [
    "# Обучение LSTM модели (lstm_train.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a599625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lstm_train.py\n",
    "Обучение LSTM модели.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from eval_lstm import evaluate_rouge_3of4\n",
    "\n",
    "\n",
    "def train_lstm(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    tokenizer,\n",
    "    device,\n",
    "    n_epochs: int = 5,\n",
    "    lr: float = 1e-3,\n",
    "    grad_clip: float = 1.0,\n",
    "    eval_every_epochs: int = 1,\n",
    "    max_new_tokens_eval: int = 32,\n",
    "    save_dir: str = \"models\",\n",
    "    save_name: str = \"lstm_best.pt\",\n",
    "):\n",
    "    model.to(device)\n",
    "\n",
    "    # labels с паддингом -100, значит ignore_index=-100\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs} [train]\"):\n",
    "            inputs = batch[\"input_ids\"].to(device)     # [B,T]\n",
    "            lengths = batch[\"lengths\"].to(device)      # [B]\n",
    "            labels = batch[\"labels\"].to(device)        # [B,T]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(inputs, lengths)            # [B,T,V]\n",
    "            # CrossEntropyLoss ждёт [N,C,*], поэтому делаем [B,V,T]\n",
    "            loss = loss_fn(logits.transpose(1, 2), labels)\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / max(1, len(train_loader))\n",
    "\n",
    "        # Валидация по loss (быстро)\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{n_epochs} [val]\"):\n",
    "                inputs = batch[\"input_ids\"].to(device)\n",
    "                lengths = batch[\"lengths\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                logits = model(inputs, lengths)\n",
    "                loss = loss_fn(logits.transpose(1, 2), labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / max(1, len(val_loader))\n",
    "\n",
    "        # ROUGE (медленнее, поэтому можно делать раз в несколько эпох)\n",
    "        rouge_str = \"\"\n",
    "        if (epoch + 1) % eval_every_epochs == 0:\n",
    "            rouge = evaluate_rouge_3of4(\n",
    "                model=model,\n",
    "                dataloader=val_loader,\n",
    "                tokenizer=tokenizer,\n",
    "                device=device,\n",
    "                max_new_tokens=max_new_tokens_eval,\n",
    "                do_sample=False,\n",
    "                top_k=None,\n",
    "                temperature=1.0,\n",
    "                # limit_batches=50,  # для теста можно включить, чтобы не ждать слишком долго\n",
    "            )\n",
    "            rouge_str = f\", ROUGE1-F={rouge['rouge1_f']:.4f}, ROUGE2-F={rouge['rouge2_f']:.4f} (n={rouge['n_samples']})\"\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}{rouge_str}\"\n",
    "        )\n",
    "\n",
    "        # -------- save best checkpoint --------\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            ckpt_path = os.path.join(save_dir, save_name)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"model_state_dict\": best_state,\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"val_loss\": best_val_loss,\n",
    "                    \"tokenizer_name\": getattr(tokenizer, \"name_or_path\", None),\n",
    "                },\n",
    "                ckpt_path,\n",
    "            )\n",
    "            print(f\"Saved best checkpoint to: {ckpt_path} (val_loss={best_val_loss:.4f})\")\n",
    "\n",
    "    # в конце загрузим лучший стейт обратно\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250487a9",
   "metadata": {},
   "source": [
    "# оценка трансформера gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5315ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kashnizky\\private\\study\\yandex\\s2\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/eval_transformer_pipeline.py\n",
    "оценка трансформера \"distilgpt2\"\n",
    "\"\"\"\n",
    "import torch\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def _decode(tokenizer, ids):\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        ids = ids.tolist()\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True).strip()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_distilgpt2_pipeline_rouge_3of4(\n",
    "    dataloader,\n",
    "    gen, # pipeline object\n",
    "    max_new_tokens_cap: int = 64,\n",
    "    do_sample: bool = True,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.95,\n",
    "    temperature: float = 0.9,\n",
    "    limit_batches: int | None = 50,\n",
    "):\n",
    "    tok = gen.tokenizer\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\"], use_stemmer=True)\n",
    "\n",
    "    total_r1, total_r2, n = 0.0, 0.0, 0\n",
    "\n",
    "    for bi, batch in enumerate(dataloader):\n",
    "        if limit_batches is not None and bi >= limit_batches:\n",
    "            break\n",
    "\n",
    "        x = batch[\"input_ids\"]      # [B,T]\n",
    "        y = batch[\"labels\"]         # [B,T]\n",
    "        lengths = batch[\"lengths\"]  # [B]\n",
    "\n",
    "        B = x.size(0)\n",
    "        for i in range(B):\n",
    "            L = int(lengths[i].item())\n",
    "            if L < 4:\n",
    "                continue\n",
    "\n",
    "            last_target = y[i, L - 1].item()\n",
    "            if last_target == -100:\n",
    "                continue\n",
    "\n",
    "            full_ids = torch.cat([x[i, :L], torch.tensor([last_target])])  # [L+1]\n",
    "            cut = max(1, int(0.75 * full_ids.numel()))\n",
    "            prefix_ids = full_ids[:cut]\n",
    "            target_ids = full_ids[cut:]\n",
    "\n",
    "            prefix_text = _decode(tok, prefix_ids)\n",
    "            ref_text = _decode(tok, target_ids)\n",
    "            if not prefix_text or not ref_text:\n",
    "                continue\n",
    "\n",
    "            gen_len = min(int(target_ids.numel()), max_new_tokens_cap)\n",
    "\n",
    "            out = gen(\n",
    "                prefix_text,\n",
    "                max_new_tokens=gen_len,          # рекомендованный способ ограничивать добавляемые токены\n",
    "                do_sample=do_sample,\n",
    "                top_k=top_k,\n",
    "                top_p=top_p,\n",
    "                temperature=temperature,\n",
    "                num_return_sequences=1,\n",
    "                return_full_text=False,          # вернуть только “добавку”, без префикса\n",
    "            )\n",
    "            hyp_text = out[0][\"generated_text\"].strip()\n",
    "\n",
    "            if not hyp_text:\n",
    "                continue\n",
    "\n",
    "            scores = scorer.score(ref_text, hyp_text)\n",
    "            total_r1 += scores[\"rouge1\"].fmeasure\n",
    "            total_r2 += scores[\"rouge2\"].fmeasure\n",
    "            n += 1\n",
    "\n",
    "    if n == 0:\n",
    "        return {\"rouge1_f\": 0.0, \"rouge2_f\": 0.0, \"n_samples\": 0}\n",
    "\n",
    "    return {\"rouge1_f\": total_r1 / n, \"rouge2_f\": total_r2 / n, \"n_samples\": n}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04fb4c",
   "metadata": {},
   "source": [
    "# Запуск всех операций executor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47168579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create cleared data file\n",
      "First 5 texts:\n",
      "0    switchfoot http twitpic com 2y1zl awww that s ...\n",
      "1    is upset that he can t update his facebook by ...\n",
      "2    kenichan i dived many times for the ball manag...\n",
      "3       my whole body feels itchy and like its on fire\n",
      "4    nationwideclass no it s not behaving at all i ...\n",
      "Name: text, dtype: object\n",
      "total length = 20000\n",
      "\n",
      "Split data files\n",
      "Lenght: train=16000, val=2000, test=2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 [train]: 100%|██████████| 63/63 [05:04<00:00,  4.83s/it]\n",
      "Epoch 1/5 [val]: 100%|██████████| 8/8 [00:15<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=8.7930, Val Loss=7.3892, ROUGE1-F=0.0478, ROUGE2-F=0.0000 (n=1920)\n",
      "Saved best checkpoint to: models\\lstm_best.pt (val_loss=7.3892)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [train]: 100%|██████████| 63/63 [05:02<00:00,  4.80s/it]\n",
      "Epoch 2/5 [val]: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=7.2491, Val Loss=7.2348, ROUGE1-F=0.0267, ROUGE2-F=0.0033 (n=1920)\n",
      "Saved best checkpoint to: models\\lstm_best.pt (val_loss=7.2348)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [train]: 100%|██████████| 63/63 [05:00<00:00,  4.77s/it]\n",
      "Epoch 3/5 [val]: 100%|██████████| 8/8 [00:14<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=7.0847, Val Loss=7.0943, ROUGE1-F=0.0332, ROUGE2-F=0.0036 (n=1920)\n",
      "Saved best checkpoint to: models\\lstm_best.pt (val_loss=7.0943)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [train]: 100%|██████████| 63/63 [05:01<00:00,  4.79s/it]\n",
      "Epoch 4/5 [val]: 100%|██████████| 8/8 [00:14<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=6.9298, Val Loss=6.9561, ROUGE1-F=0.0649, ROUGE2-F=0.0050 (n=1920)\n",
      "Saved best checkpoint to: models\\lstm_best.pt (val_loss=6.9561)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [train]: 100%|██████████| 63/63 [05:12<00:00,  4.97s/it]\n",
      "Epoch 5/5 [val]: 100%|██████████| 8/8 [00:15<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=6.7795, Val Loss=6.8307, ROUGE1-F=0.0673, ROUGE2-F=0.0053 (n=1920)\n",
      "Saved best checkpoint to: models\\lstm_best.pt (val_loss=6.8307)\n",
      "LSTM VAL: {'rouge1_f': 0.06725882760349697, 'rouge2_f': 0.005282486610611611, 'n_samples': 1920}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 VAL: {'rouge1_f': 0.06550649469167424, 'rouge2_f': 0.006085652538621535, 'n_samples': 1903}\n",
      "PREFIX: i am going\n",
      "LSTM +: to it in the time is of up one 3 we it is in work awww to work for\n",
      "GPT2 +: to be.”\n",
      "\n",
      "[image via Getty Images]\n",
      "------------------------------------------------------------\n",
      "PREFIX: tomorrow i will\n",
      "LSTM +: the you i was from the morning good not was but it i and now i so he but i\n",
      "GPT2 +: keep this for the long term.\n",
      "But if you have any questions, then just write this:\n",
      "------------------------------------------------------------\n",
      "PREFIX: this movie is\n",
      "LSTM +: in the i was out i have i don t am was to the and of s day no i\n",
      "GPT2 +: a pretty good one as I haven't heard much about it before.) I'm very impressed with this\n",
      "------------------------------------------------------------\n",
      "PREFIX: Company Google is\n",
      "LSTM +: it is my now it is too it be good day is so i miss you could feel i think\n",
      "GPT2 +: making changes to its search engine and search engine to replace its search engine for the Google Search engine and\n",
      "------------------------------------------------------------\n",
      "PREFIX: If you compare Google and Yandex, you could say that\n",
      "LSTM +: of my bad at work not have to get to this in twitter i would it 2 all back to\n",
      "GPT2 +: Yandex uses the same techniques as Google, which relies on artificial intelligence to make the pages for\n",
      "------------------------------------------------------------\n",
      "PREFIX: Our mentor is very smart and\n",
      "LSTM +: that at a great have a it like work so on you to be is out and you it so\n",
      "GPT2 +: has a lot of experience in making sure that your program is not running at all.\n",
      "------------------------------------------------------------\n",
      "PREFIX: The distilgpt2 model is very strange, but it allows\n",
      "LSTM +: no too lol t really t have i can t be in worky out s a to on your\n",
      "GPT2 +: the production process to take place.\n",
      "\n",
      "But its own internal combustion engine will work for the engine\n",
      "------------------------------------------------------------\n",
      "PREFIX: I would like\n",
      "LSTM +: for the time on for in the that to the i t go too me it s the i would\n",
      "GPT2 +: to thank Mr. B. Kallerman for his thoughtful work, particularly his outstanding contributions, the\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# from data_utils import prepare_all_data\n",
    "# from next_token_dataset import GenDataLoaders\n",
    "# from lstm_model import LSTMNextToken\n",
    "# from lstm_train import train_lstm\n",
    "# from eval_lstm import evaluate_rouge_3of4\n",
    "# from eval_transformer_pipeline import evaluate_distilgpt2_pipeline_rouge_3of4\n",
    "\n",
    "debug_test = True\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 1) Tokenizer\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# GPT2 обычно без PAD\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "pad_id = tokenizer.pad_token_id or 0\n",
    "eos_id = tokenizer.eos_token_id\n",
    "\n",
    "# 2) Devices\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipeline_device = 0 if torch.cuda.is_available() else -1  # pipeline ждёт int/-1\n",
    "\n",
    "# 3) Data\n",
    "train_df, val_df, test_df = prepare_all_data(force=True, debug_test=debug_test)\n",
    "train_loader, val_loader, test_loader = GenDataLoaders(train_df, val_df, test_df, tokenizer)\n",
    "\n",
    "# 4) LSTM train\n",
    "model = LSTMNextToken(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,\n",
    "    hidden_size=128,\n",
    "    num_layers=1,\n",
    "    dropout=0.1,\n",
    "    pad_id=pad_id,\n",
    ")\n",
    "\n",
    "model = train_lstm(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    n_epochs=5,\n",
    "    lr=1e-3,\n",
    "    grad_clip=1.0,\n",
    "    eval_every_epochs=1,\n",
    "    max_new_tokens_eval=20,\n",
    ")\n",
    "\n",
    "# если потом нужно загрузить модель для инференса\n",
    "# ckpt = torch.load(\"models/lstm_best.pt\", map_location=device)\n",
    "# model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "# model.eval()\n",
    "\n",
    "rouge_lstm_val = evaluate_rouge_3of4(\n",
    "    model, val_loader, tokenizer, device,\n",
    "    max_new_tokens=32,\n",
    "    do_sample=False\n",
    ")\n",
    "print(\"LSTM VAL:\", rouge_lstm_val)\n",
    "\n",
    "# 5) distilgpt2 pipeline (ONE instance, reuse everywhere)\n",
    "gpt2_gen = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    tokenizer=tokenizer,   # тот же объект токенизатора для надежности\n",
    "    device=pipeline_device,\n",
    ")\n",
    "\n",
    "# 6) distilgpt2 eval\n",
    "rouge_gpt2_val = evaluate_distilgpt2_pipeline_rouge_3of4(\n",
    "    dataloader=val_loader,\n",
    "    gen=gpt2_gen,\n",
    "    max_new_tokens_cap=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.9,\n",
    "    limit_batches=None,\n",
    ")\n",
    "print(\"GPT2 VAL:\", rouge_gpt2_val)\n",
    "\n",
    "\n",
    "# 7) Side-by-side examples\n",
    "list_examples = [\n",
    "    \"i am going\",\n",
    "    \"tomorrow i will\",\n",
    "    \"this movie is\",\n",
    "    \"Company Google is\",\n",
    "    \"If you compare Google and Yandex, you could say that\",\n",
    "    \"Our mentor is very smart and\",\n",
    "    \"The distilgpt2 model is very strange, but it allows\",\n",
    "    \"I would like\",\n",
    "]\n",
    "\n",
    "for prefix in list_examples:\n",
    "    # LSTM: генерим ids и декодим\n",
    "    prefix_ids = tokenizer(prefix, add_special_tokens=False)[\"input_ids\"]\n",
    "    prefix_ids = torch.tensor(prefix_ids, dtype=torch.long, device=device)\n",
    "\n",
    "    lstm_ids = model.generate(\n",
    "        prefix_ids,\n",
    "        max_new_tokens=20,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        temperature=1.0,\n",
    "        eos_id=eos_id,\n",
    "    )\n",
    "    lstm_full = tokenizer.decode(lstm_ids.tolist(), skip_special_tokens=True)\n",
    "    lstm_suffix = lstm_full[len(prefix):].strip()\n",
    "\n",
    "    # GPT2 pipeline: сразу просим только continuation\n",
    "    out = gpt2_gen(\n",
    "        prefix,\n",
    "        max_new_tokens=20,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.9,\n",
    "        num_return_sequences=1,\n",
    "        return_full_text=False,\n",
    "    )\n",
    "    gpt2_suffix = out[0][\"generated_text\"].strip()\n",
    "\n",
    "    print(\"PREFIX:\", prefix)\n",
    "    print(\"LSTM +:\", lstm_suffix)\n",
    "    print(\"GPT2 +:\", gpt2_suffix)\n",
    "    print(\"-\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30154e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
