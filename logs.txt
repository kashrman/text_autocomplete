
#########################
В данном файле зафиксированы метрики и примеры, которые получились при обучении
#########################

###
# последняя версия, полный набор
###
Epoch 1: Train Loss=5.7429, Val Loss=5.3724, ROUGE1-F=0.0930, ROUGE2-F=0.0164 (n=154163)
Epoch 2: Train Loss=5.2617, Val Loss=5.2269, ROUGE1-F=0.1000, ROUGE2-F=0.0189 (n=154163)
Epoch 3: Train Loss=5.1423, Val Loss=5.1653, ROUGE1-F=0.1012, ROUGE2-F=0.0197 (n=154163)
Epoch 4: Train Loss=5.0752, Val Loss=5.1305, ROUGE1-F=0.1031, ROUGE2-F=0.0207 (n=154163)
Epoch 5: Train Loss=5.0299, Val Loss=5.1092, ROUGE1-F=0.1042, ROUGE2-F=0.0211 (n=154163)
Epoch 6: Train Loss=4.9972, Val Loss=5.0949, ROUGE1-F=0.1048, ROUGE2-F=0.0216 (n=154163)
Epoch 7: Train Loss=4.9716, Val Loss=5.0850, ROUGE1-F=0.1049, ROUGE2-F=0.0215 (n=154163)
Epoch 8: Train Loss=4.9511, Val Loss=5.0779, ROUGE1-F=0.1071, ROUGE2-F=0.0222 (n=154163)

LSTM VAL: {'rouge1_f': 0.10711065520183888, 'rouge2_f': 0.022228194860246516, 'n_samples': 154163}
GPT2 VAL: {'rouge1_f': 0.05737843178123644, 'rouge2_f': 0.004854144314164569, 'n_samples': 152545}

Примеры (при max_new_tokens=5):

PREFIX: i am going
LSTM +: to get to the beach
GPT2 +: to find a way to

PREFIX: tomorrow i will
LSTM +: be leaving tomorrow for finals
GPT2 +: make this one an issue

PREFIX: this movie is
LSTM +: for me for my last
GPT2 +: about a young man who

PREFIX: Company Google is
LSTM +: a kid with us i
GPT2 +: in its third quarter,

PREFIX: If you compare Google and Yandex, you could say that
LSTM +: and we can t go
GPT2 +: Google is the biggest influence

PREFIX: Our mentor is very smart and
LSTM +: we can be here i
GPT2 +: very passionate in his work

PREFIX: The distilgpt2 model is very strange, but it allows
...
PREFIX: I would like
LSTM +: me 2 my family i
GPT2 +: to thank the community for


####
# первая версия при обучении на 20000 строк:
####
Epoch 1/5 [train]: 100%|██████████| 63/63 [05:16<00:00, 5.02s/it]
Epoch 1/5 [val]: 100%|██████████| 8/8 [00:14<00:00, 1.87s/it]
Epoch 1: Train Loss=8.7930, Val Loss=7.3892, ROUGE1-F=0.0478, ROUGE2-F=0.0000 (n=1920) 

Epoch 2/5 [train]: 100%|██████████| 63/63 [05:25<00:00, 5.16s/it]
Epoch 2/5 [val]: 100%|██████████| 8/8 [00:15<00:00, 1.92s/it]

Epoch 2: Train Loss=7.2491, Val Loss=7.2348, ROUGE1-F=0.0267, ROUGE2-F=0.0033 (n=1920) 

Epoch 3/5 [train]: 100%|██████████| 63/63 [05:05<00:00, 4.85s/it]
Epoch 3/5 [val]: 100%|██████████| 8/8 [00:14<00:00, 1.86s/it]

Epoch 3: Train Loss=7.0847, Val Loss=7.0943, ROUGE1-F=0.0332, ROUGE2-F=0.0036 (n=1920) 

Epoch 4/5 [train]: 100%|██████████| 63/63 [05:03<00:00, 4.81s/it]
Epoch 4/5 [val]: 100%|██████████| 8/8 [00:15<00:00, 1.96s/it]

Epoch 4: Train Loss=6.9298, Val Loss=6.9561, ROUGE1-F=0.0649, ROUGE2-F=0.0050 (n=1920) 

Epoch 5/5 [train]: 100%|██████████| 63/63 [05:06<00:00, 4.86s/it]
Epoch 5/5 [val]: 100%|██████████| 8/8 [00:14<00:00, 1.87s/it]

Epoch 5: Train Loss=6.7795, Val Loss=6.8307, ROUGE1-F=0.0673, ROUGE2-F=0.0053 (n=1920) 

LSTM VAL: {'rouge1_f': 0.06725882760349697, 'rouge2_f': 0.005282486610611611, 'n_samples': 1920}

GPT2 VAL: {'rouge1_f': 0.06550649469167424, 'rouge2_f': 0.006085652538621535, 'n_samples': 1903}

PREFIX: i am going
LSTM +: to it in the time is of up one 3 we it is in work awww to work for
GPT2 +: to be.”

PREFIX: tomorrow i will
LSTM +: the you i was from the morning good not was but it i and now i so he but i
GPT2 +: keep this for the long term.
But if you have any questions, then just write this:

PREFIX: this movie is
LSTM +: in the i was out i have i don t am was to the and of s day no i
GPT2 +: a pretty good one as I haven't heard much about it before.) I'm very impressed with this

PREFIX: Company Google is
LSTM +: it is my now it is too it be good day is so i miss you could feel i think
GPT2 +: making changes to its search engine and search engine to replace its search engine for the Google Search engine and

PREFIX: If you compare Google and Yandex, you could say that
LSTM +: of my bad at work not have to get to this in twitter i would it 2 all back to
GPT2 +: Yandex uses the same techniques as Google, which relies on artificial intelligence to make the pages for

PREFIX: I would like
LSTM +: for the time on for in the that to the i t go too me it s the i would
GPT2 +: to thank Mr. B. Kallerman for his thoughtful work, particularly his outstanding contributions, the


###
# вторая версия, полный набор
###

Epoch 1/10 [train]: 100%|██████████| 10004/10004 [30:22<00:00,  5.49it/s]
Epoch 1/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.78it/s]
Epoch 1: Train Loss=5.7429, Val Loss=5.3724, ROUGE1-F=0.0930, ROUGE2-F=0.0164 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.3724)
Epoch 2/10 [train]: 100%|██████████| 10004/10004 [30:24<00:00,  5.48it/s]
Epoch 2/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.76it/s]
Epoch 2: Train Loss=5.2617, Val Loss=5.2269, ROUGE1-F=0.1000, ROUGE2-F=0.0189 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.2269)
Epoch 3/10 [train]: 100%|██████████| 10004/10004 [30:26<00:00,  5.48it/s]
Epoch 3/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.78it/s]
Epoch 3: Train Loss=5.1423, Val Loss=5.1653, ROUGE1-F=0.1012, ROUGE2-F=0.0197 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.1653)
Epoch 4/10 [train]: 100%|██████████| 10004/10004 [30:23<00:00,  5.49it/s]
Epoch 4/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.76it/s]
Epoch 4: Train Loss=5.0752, Val Loss=5.1305, ROUGE1-F=0.1031, ROUGE2-F=0.0207 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.1305)
Epoch 5/10 [train]: 100%|██████████| 10004/10004 [30:24<00:00,  5.48it/s]
Epoch 5/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.77it/s]
Epoch 5: Train Loss=5.0299, Val Loss=5.1092, ROUGE1-F=0.1042, ROUGE2-F=0.0211 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.1092)
Epoch 6/10 [train]: 100%|██████████| 10004/10004 [30:25<00:00,  5.48it/s]
Epoch 6/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.76it/s]
Epoch 6: Train Loss=4.9972, Val Loss=5.0949, ROUGE1-F=0.1048, ROUGE2-F=0.0216 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.0949)
Epoch 7/10 [train]: 100%|██████████| 10004/10004 [30:23<00:00,  5.49it/s]
Epoch 7/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.79it/s]
Epoch 7: Train Loss=4.9716, Val Loss=5.0850, ROUGE1-F=0.1049, ROUGE2-F=0.0215 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.0850)
Epoch 8/10 [train]: 100%|██████████| 10004/10004 [30:21<00:00,  5.49it/s]
Epoch 8/10 [val]: 100%|██████████| 1251/1251 [01:14<00:00, 16.78it/s]
Epoch 8: Train Loss=4.9511, Val Loss=5.0779, ROUGE1-F=0.1071, ROUGE2-F=0.0222 (n=154163)
Saved best checkpoint to: models/lstm_best.pt (val_loss=5.0779)
